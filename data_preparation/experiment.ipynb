{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('.'))\n",
    "\n",
    "# Change from relative to absolute imports\n",
    "from dataset_preparation import TauPreparer\n",
    "import data_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_train = {\n",
    "     \"cortical\": ['747316', '771746', '771791'],\n",
    "     \"BG\": ['747814', '747818', '747297', '747309'],\n",
    "     \"DN\": ['747337','747350','747352']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "d = {\n",
    "     \"cortical\": ['747331','771747'],\n",
    "     \"BG\": ['703488','747821'],\n",
    "     \"DN\": ['747335','771913']\n",
    "}\n",
    "object_labels = {}\n",
    "\n",
    "in_root_dir = \"M:/Unused/TauCellDL/labels/test\"\n",
    "subdirs = os.listdir(in_root_dir)\n",
    "for subdir in subdirs:\n",
    "    subdir_path = os.path.join(in_root_dir, subdir)\n",
    "    detection_files = data_utils.list_files_of_a_type(subdir_path, \"txt\")\n",
    "\n",
    "    for i in tqdm.tqdm(range(len(detection_files))):\n",
    "        detection_file = detection_files[i]\n",
    "        print(detection_file)\n",
    "\n",
    "        # print(f\"\\nProcessing {detection_file}...\")\n",
    "        split_filename = data_utils.get_filename(detection_file).split(' ')[0].split('.')\n",
    "        slide_id = split_filename[0]\n",
    "        # print(slide_id)\n",
    "        # find key for slide_id\n",
    "        key = next((key for key, value in d.items() if slide_id in value), None)\n",
    "        if key is None:\n",
    "            continue\n",
    "        if key not in object_labels:\n",
    "            object_labels[key] = {}\n",
    "        with open(detection_file, 'r') as detections:\n",
    "            for line in detections:\n",
    "                object_label = line.split(' ')[0].strip().strip(']').strip('[')\n",
    "                object_labels[key][object_label] = object_labels[key].get(object_label, 0) + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DN': {'3': 919, '2': 44, '1': 24},\n",
       " 'cortical': {'2': 106, '1': 355, '0': 369, '3': 1914},\n",
       " 'BG': {'0': 81, '1': 199, '3': 3064, '2': 33}}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "object_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 2/6 [00:00<00:00, 19.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing M:/Unused/TauCellDL/test_labels_new\\703488_detections_filtered.txt...\n",
      "\n",
      "Processing M:/Unused/TauCellDL/test_labels_new\\747331_detections_filtered.txt...\n",
      "\n",
      "Processing M:/Unused/TauCellDL/test_labels_new\\747335_detections_filtered.txt...\n",
      "\n",
      "Processing M:/Unused/TauCellDL/test_labels_new\\747821_detections_filtered.txt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 16.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing M:/Unused/TauCellDL/test_labels_new\\771747_detections_filtered.txt...\n",
      "\n",
      "Processing M:/Unused/TauCellDL/test_labels_new\\771913_detections_filtered.txt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "import os\n",
    "\n",
    "in_root_dir = \"M:/Unused/TauCellDL/test_labels_new\"\n",
    "\n",
    "detection_files = data_utils.list_files_of_a_type(in_root_dir, \"txt\")\n",
    "\n",
    "d = {\n",
    "     \"cortical\": ['747331','771747'],\n",
    "     \"BG\": ['703488', '747821'],\n",
    "     \"DN\": ['747335','771913']\n",
    "}\n",
    "object_labels = {}\n",
    "\n",
    "for i in tqdm.tqdm(range(len(detection_files))):\n",
    "    detection_file = detection_files[i]\n",
    "\n",
    "    print(f\"\\nProcessing {detection_file}...\")\n",
    "    split_filename = data_utils.get_filename(detection_file).split('_')\n",
    "    slide_id = split_filename[0]\n",
    "    # find key for slide_id\n",
    "    key = next((key for key, value in d.items() if slide_id in value), None)\n",
    "    if key is None:\n",
    "        continue\n",
    "    if key not in object_labels:\n",
    "        object_labels[key] = {}\n",
    "    with open(detection_file, 'r') as detections:\n",
    "        for line in detections:\n",
    "                object_label = line.split(':')[0].strip().strip(']').strip('[')\n",
    "                object_labels[key][object_label] = object_labels[key].get(object_label, 0) + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BG': {'tau_fragments': 2796, 'coiled': 159, 'NFT': 21, 'TA': 49},\n",
       " 'cortical': {'CB': 175,\n",
       "  'TA': 237,\n",
       "  'NFT': 78,\n",
       "  'tau_fragments': 1761,\n",
       "  'coiled': 121},\n",
       " 'DN': {'Others': 845, 'NFT': 32, 'CB': 18}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "object_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# for files in cortical and DN regions, filter out all objects with label 'tau_fragments'\n",
    "in_root_dir = \"M:/Unused/TauCellDL/labels_new\"\n",
    "\n",
    "detection_files = data_utils.list_files_of_a_type(in_root_dir, \".txt\")\n",
    "\n",
    "d = {\n",
    "     \"cortical\": ['747316', '771746', '771791'],\n",
    "     \"BG\": ['747814', '747818', '747297', '747309'],\n",
    "     \"DN\": ['747337','747350','747352']\n",
    "}\n",
    "\n",
    "for i in tqdm.tqdm(range(len(detection_files))):\n",
    "    detection_file = detection_files[i]\n",
    "\n",
    "    print(f\"\\nProcessing {detection_file}...\")\n",
    "    split_filename = data_utils.get_filename(detection_file).split('_')\n",
    "    slide_id = split_filename[0]\n",
    "    # find key for slide_id\n",
    "    key = next((key for key, value in d.items() if slide_id in value), None)\n",
    "    if key is None or key != \"BG\":\n",
    "        continue\n",
    "    lines = []\n",
    "    with open(detection_file, 'r') as detections:\n",
    "        for line in detections:     \n",
    "                object_label = line.split(':')[0].strip().strip(']').strip('[')\n",
    "                if object_label == \"CB\":\n",
    "                     continue\n",
    "                else:\n",
    "                     lines.append(line)\n",
    "    with open(detection_file, 'w') as f:\n",
    "        for line in lines:\n",
    "            f.write(line)\n",
    "                     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# for files in cortical and DN regions, rename label 'Others' to 'tau_fragments'\n",
    "in_root_dir = \"M:/Unused/TauCellDL/labels_new\"\n",
    "\n",
    "detection_files = data_utils.list_files_of_a_type(in_root_dir, \".txt\")\n",
    "\n",
    "d = {\n",
    "     \"cortical\": ['747316', '771746', '771791'],\n",
    "     \"BG\": ['747814', '747818', '747297', '747309'],\n",
    "     \"DN\": ['747337','747350','747352']\n",
    "}\n",
    "\n",
    "for i in tqdm.tqdm(range(len(detection_files))):\n",
    "    detection_file = detection_files[i]\n",
    "\n",
    "    print(f\"\\nProcessing {detection_file}...\")\n",
    "    split_filename = data_utils.get_filename(detection_file).split('_')\n",
    "    slide_id = split_filename[0]\n",
    "    # find key for slide_id\n",
    "    key = next((key for key, value in d.items() if slide_id in value), None)\n",
    "    if key is None or key == \"BG\":\n",
    "        continue\n",
    "    lines = []\n",
    "    with open(detection_file, 'r') as detections:\n",
    "        for line in detections:     \n",
    "                object_label = line.split(':')[0].strip().strip(']').strip('[')\n",
    "                if object_label == \"Others\":\n",
    "                    object_label = \"tau_fragments\"\n",
    "                lines.append(object_label + \":\" + line.split(':')[1])\n",
    "    with open(detection_file, 'w') as f:\n",
    "        for line in lines:\n",
    "            f.write(line)\n",
    "                     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# for files in cortical and DN regions, rename label 'Others' to 'tau_fragments'\n",
    "in_root_dir = \"M:/Unused/TauCellDL/labels_new\"\n",
    "\n",
    "detection_files = data_utils.list_files_of_a_type(in_root_dir, \".txt\")\n",
    "\n",
    "d = {\n",
    "     \"cortical\": ['747316', '771746', '771791'],\n",
    "     \"BG\": ['747814', '747818', '747297', '747309'],\n",
    "     \"DN\": ['747337','747350','747352']\n",
    "}\n",
    "\n",
    "for i in tqdm.tqdm(range(len(detection_files))):\n",
    "    detection_file = detection_files[i]\n",
    "\n",
    "    print(f\"\\nProcessing {detection_file}...\")\n",
    "    split_filename = data_utils.get_filename(detection_file).split('_')\n",
    "    slide_id = split_filename[0]\n",
    "    # find key for slide_id\n",
    "    key = next((key for key, value in d.items() if slide_id in value), None)\n",
    "    if key is None or key != \"BG\":\n",
    "        continue\n",
    "    lines = []\n",
    "    with open(detection_file, 'r') as detections:\n",
    "        for line in detections:     \n",
    "                object_label = line.split(':')[0].strip().strip(']').strip('[')\n",
    "                if object_label == \"coiled\":\n",
    "                    object_label = \"CB\"\n",
    "                lines.append(object_label + \":\" + line.split(':')[1])\n",
    "    with open(detection_file, 'w') as f:\n",
    "        for line in lines:\n",
    "            f.write(line)\n",
    "                     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import seaborn as sns\n",
    "\n",
    "def get_object_sizes_by_region():\n",
    "    in_root_dir = \"M:/Unused/TauCellDL/labels_new\"\n",
    "    detection_files = data_utils.list_files_of_a_type(in_root_dir, \".txt\")\n",
    "    \n",
    "    # Dictionary to store sizes: region -> class -> list of sizes\n",
    "    sizes = {}\n",
    "    \n",
    "    d = {\n",
    "         \"cortical\": ['747316', '771746', '771791'],\n",
    "         \"BG\": ['747814', '747818', '747297', '747309'],\n",
    "         \"DN\": ['747337','747350','747352']\n",
    "    }\n",
    "    \n",
    "    for detection_file in detection_files:\n",
    "        split_filename = data_utils.get_filename(detection_file).split('_')\n",
    "        slide_id = split_filename[0]\n",
    "        # find key for slide_id\n",
    "        key = next((key for key, value in d.items() if slide_id in value), None)\n",
    "        if key is None:\n",
    "            continue\n",
    "        if key not in sizes:\n",
    "            sizes[key] = {}\n",
    "            \n",
    "        with open(detection_file, 'r') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split(':')\n",
    "                if len(parts) != 2:\n",
    "                    continue\n",
    "                    \n",
    "                object_label = parts[0].strip().strip(']').strip('[')\n",
    "                bbox = (parts[1].split(' ',2))[2].strip().strip('(').strip(')')\n",
    "                # Parse bbox coordinates (assuming format: x,y,w,h)\n",
    "                try:\n",
    "                    coords = [int(x) for x in bbox.split(',')]\n",
    "                    print(coords)\n",
    "                    if len(coords) >= 4:\n",
    "                        area = math.sqrt(coords[2] * coords[3])  # width * height\n",
    "                        if object_label not in sizes[key]:\n",
    "                            sizes[key][object_label] = []\n",
    "                        sizes[key][object_label].append(area)\n",
    "                except Exception as e:\n",
    "                    # print error\n",
    "                    print(e)\n",
    "\n",
    "                    continue\n",
    "    \n",
    "    return sizes\n",
    "\n",
    "def plot_size_distributions(sizes):\n",
    "    # Set up the plot style\n",
    "    sns.set_palette(\"husl\")\n",
    "    \n",
    "    # Create subplots for each region\n",
    "    regions = list(sizes.keys())\n",
    "    fig, axes = plt.subplots(len(regions), 1, figsize=(12, 5*len(regions)))\n",
    "    fig.suptitle('Object Size Distributions by Region and Class', fontsize=16)\n",
    "    \n",
    "    for idx, region in enumerate(regions):\n",
    "        ax = axes[idx] if len(regions) > 1 else axes\n",
    "        \n",
    "        # Prepare data for boxplot\n",
    "        data = []\n",
    "        labels = []\n",
    "        for class_name, class_sizes in sizes[region].items():\n",
    "            if len(class_sizes) > 0:  # Only include if we have data\n",
    "                data.append(class_sizes)\n",
    "                labels.append(f\"{class_name}\\n(n={len(class_sizes)})\")\n",
    "        \n",
    "        # Create box plot\n",
    "        ax.boxplot(data, labels=labels)\n",
    "        ax.set_title(f'{region} Region')\n",
    "        ax.set_ylabel('Object Area (pixels²)')\n",
    "        ax.set_yscale('log')  # Log scale often works better for size distributions\n",
    "        \n",
    "        # Add some grid lines\n",
    "        ax.grid(True, linestyle='--', alpha=0.7)\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Execute the visualization\n",
    "sizes = get_object_sizes_by_region()\n",
    "sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot by type of label\n",
    "def plot_size_distributions_by_label(sizes):\n",
    "    labels = {}\n",
    "    for type_object in sizes[\"cortical\"].keys():\n",
    "        labels[type_object] = []\n",
    "    for dic in sizes.values():\n",
    "        for type_object in dic.keys():\n",
    "            if type_object not in labels:\n",
    "                labels[\"CB\"].extend(dic[type_object])\n",
    "            else:\n",
    "                labels[type_object].extend(dic[type_object])\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    means = []\n",
    "    label_names = []\n",
    "    for label, values in labels.items():\n",
    "        mean_value = np.mean(values)\n",
    "        means.append(mean_value)\n",
    "        label_names.append(label)\n",
    "        print(f\"{label}: {mean_value}\")\n",
    "    \n",
    "    plt.bar(label_names, means)\n",
    "    # Or if you really want a histogram:\n",
    "    # plt.hist(means, bins=len(means), alpha=0.7, label='Labels')\n",
    "    \n",
    "    plt.xlabel('Label Type')\n",
    "    plt.ylabel('Mean Size')\n",
    "    plt.title('Mean Size by Label Type')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Number of rows must be a positive integer, not 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mplot_size_distributions\u001b[49m\u001b[43m(\u001b[49m\u001b[43msizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# plot_size_distributions_by_label(sizes)\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Optional: Print summary statistics\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m region, class_sizes \u001b[38;5;129;01min\u001b[39;00m sizes\u001b[38;5;241m.\u001b[39mitems():\n",
      "Cell \u001b[1;32mIn[40], line 62\u001b[0m, in \u001b[0;36mplot_size_distributions\u001b[1;34m(sizes)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# Create subplots for each region\u001b[39;00m\n\u001b[0;32m     61\u001b[0m regions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(sizes\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m---> 62\u001b[0m fig, axes \u001b[38;5;241m=\u001b[39m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubplots\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mregions\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfigsize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mregions\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     63\u001b[0m fig\u001b[38;5;241m.\u001b[39msuptitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mObject Size Distributions by Region and Class\u001b[39m\u001b[38;5;124m'\u001b[39m, fontsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, region \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(regions):\n",
      "File \u001b[1;32mc:\\Users\\peiya\\Desktop\\dissertation\\dl_histopathology\\.venv\\Lib\\site-packages\\matplotlib\\pyplot.py:1760\u001b[0m, in \u001b[0;36msubplots\u001b[1;34m(nrows, ncols, sharex, sharey, squeeze, width_ratios, height_ratios, subplot_kw, gridspec_kw, **fig_kw)\u001b[0m\n\u001b[0;32m   1615\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1616\u001b[0m \u001b[38;5;124;03mCreate a figure and a set of subplots.\u001b[39;00m\n\u001b[0;32m   1617\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \n\u001b[0;32m   1758\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m fig \u001b[38;5;241m=\u001b[39m figure(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfig_kw)\n\u001b[1;32m-> 1760\u001b[0m axs \u001b[38;5;241m=\u001b[39m \u001b[43mfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubplots\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnrows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mncols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mncols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msharex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msharey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1761\u001b[0m \u001b[43m                   \u001b[49m\u001b[43msqueeze\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubplot_kw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubplot_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1762\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mgridspec_kw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgridspec_kw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheight_ratios\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheight_ratios\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1763\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mwidth_ratios\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwidth_ratios\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1764\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fig, axs\n",
      "File \u001b[1;32mc:\\Users\\peiya\\Desktop\\dissertation\\dl_histopathology\\.venv\\Lib\\site-packages\\matplotlib\\figure.py:860\u001b[0m, in \u001b[0;36mFigureBase.subplots\u001b[1;34m(self, nrows, ncols, sharex, sharey, squeeze, width_ratios, height_ratios, subplot_kw, gridspec_kw)\u001b[0m\n\u001b[0;32m    856\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwidth_ratios\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must not be defined both as \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    857\u001b[0m                          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter and as key in \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgridspec_kw\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    858\u001b[0m     gridspec_kw[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwidth_ratios\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m width_ratios\n\u001b[1;32m--> 860\u001b[0m gs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_gridspec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mncols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfigure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgridspec_kw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    861\u001b[0m axs \u001b[38;5;241m=\u001b[39m gs\u001b[38;5;241m.\u001b[39msubplots(sharex\u001b[38;5;241m=\u001b[39msharex, sharey\u001b[38;5;241m=\u001b[39msharey, squeeze\u001b[38;5;241m=\u001b[39msqueeze,\n\u001b[0;32m    862\u001b[0m                   subplot_kw\u001b[38;5;241m=\u001b[39msubplot_kw)\n\u001b[0;32m    863\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m axs\n",
      "File \u001b[1;32mc:\\Users\\peiya\\Desktop\\dissertation\\dl_histopathology\\.venv\\Lib\\site-packages\\matplotlib\\figure.py:1538\u001b[0m, in \u001b[0;36mFigureBase.add_gridspec\u001b[1;34m(self, nrows, ncols, **kwargs)\u001b[0m\n\u001b[0;32m   1495\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;124;03mLow-level API for creating a `.GridSpec` that has this figure as a parent.\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1534\u001b[0m \n\u001b[0;32m   1535\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m _ \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfigure\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# pop in case user has added this...\u001b[39;00m\n\u001b[1;32m-> 1538\u001b[0m gs \u001b[38;5;241m=\u001b[39m \u001b[43mGridSpec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnrows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mncols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mncols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfigure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1539\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m gs\n",
      "File \u001b[1;32mc:\\Users\\peiya\\Desktop\\dissertation\\dl_histopathology\\.venv\\Lib\\site-packages\\matplotlib\\gridspec.py:363\u001b[0m, in \u001b[0;36mGridSpec.__init__\u001b[1;34m(self, nrows, ncols, figure, left, bottom, right, top, wspace, hspace, width_ratios, height_ratios)\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhspace \u001b[38;5;241m=\u001b[39m hspace\n\u001b[0;32m    361\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure \u001b[38;5;241m=\u001b[39m figure\n\u001b[1;32m--> 363\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mncols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    364\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mwidth_ratios\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwidth_ratios\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    365\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mheight_ratios\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheight_ratios\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\peiya\\Desktop\\dissertation\\dl_histopathology\\.venv\\Lib\\site-packages\\matplotlib\\gridspec.py:48\u001b[0m, in \u001b[0;36mGridSpecBase.__init__\u001b[1;34m(self, nrows, ncols, height_ratios, width_ratios)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;124;03m----------\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;124;03m    If not given, all rows will have the same height.\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(nrows, Integral) \u001b[38;5;129;01mor\u001b[39;00m nrows \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 48\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     49\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of rows must be a positive integer, not \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnrows\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ncols, Integral) \u001b[38;5;129;01mor\u001b[39;00m ncols \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     52\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of columns must be a positive integer, not \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mncols\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Number of rows must be a positive integer, not 0"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x0 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_size_distributions(sizes)\n",
    "# plot_size_distributions_by_label(sizes)\n",
    "\n",
    "# Optional: Print summary statistics\n",
    "for region, class_sizes in sizes.items():\n",
    "    print(f\"\\n{region} Region Statistics:\")\n",
    "    for class_name, size_list in class_sizes.items():\n",
    "        if len(size_list) > 0:\n",
    "            print(f\"\\n{class_name}:\")\n",
    "            print(f\"  Count: {len(size_list)}\")\n",
    "            print(f\"  Mean: {np.mean(size_list):.2f}\")\n",
    "            print(f\"  Median: {np.median(size_list):.2f}\")\n",
    "            print(f\"  Std: {np.std(size_list):.2f}\")\n",
    "            print(f\"  Min: {np.min(size_list):.2f}\")\n",
    "            print(f\"  Max: {np.max(size_list):.2f}\")\n",
    "\n",
    "# output summary statistics to a csv file, include the column names too \n",
    "with open(\"M:/Unused/TauCellDL/object_size_statistics.csv\", \"w\") as f:\n",
    "    f.write(\"Region,Class,Mean,Median,Std,Min,Max\\n\")\n",
    "    for region, class_sizes in sizes.items():\n",
    "        for class_name, size_list in class_sizes.items():\n",
    "            f.write(f\"{region},{class_name},{np.mean(size_list):.2f},{np.median(size_list):.2f},{np.std(size_list):.2f},{np.min(size_list):.2f},{np.max(size_list):.2f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing region \n",
      "\n",
      "Creating filtered detection lists...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing M:/Unused/TauCellDL\\labels\\747297_detections.txt...\n",
      "\n",
      "Separating labels by tile for each slide...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[118], line 7\u001b[0m\n\u001b[0;32m      3\u001b[0m in_root_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mM:/Unused/TauCellDL\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      5\u001b[0m data_preparer \u001b[38;5;241m=\u001b[39m TauPreparer(in_root_dir\u001b[38;5;241m=\u001b[39min_root_dir, in_img_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m\"\u001b[39m, in_label_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      6\u001b[0m                             prepared_root_dir\u001b[38;5;241m=\u001b[39min_root_dir, prepared_img_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages_new\u001b[39m\u001b[38;5;124m\"\u001b[39m, prepared_label_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels_new\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m \u001b[43mdata_preparer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_labels_for_yolo\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\peiya\\Desktop\\dissertation\\dl_histopathology\\data_preparation\\dataset_preparation.py:375\u001b[0m, in \u001b[0;36mTauPreparer.prepare_labels_for_yolo\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    372\u001b[0m label_preparer\u001b[38;5;241m.\u001b[39mremove_unlabelled_objects()\n\u001b[0;32m    374\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mSeparating labels by tile for each slide...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 375\u001b[0m \u001b[43mlabel_preparer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseparate_labels_by_tile\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mDeleting images/labels with empty label files...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\peiya\\Desktop\\dissertation\\dl_histopathology\\data_preparation\\qupath_label_preparation.py:133\u001b[0m, in \u001b[0;36mseparate_labels_by_tile\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "File \u001b[1;32mc:\\Users\\peiya\\Desktop\\dissertation\\dl_histopathology\\data_preparation\\data_utils.py:19\u001b[0m, in \u001b[0;36mlist_files_of_a_type\u001b[1;34m(directory, file_extension)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlist_files_of_a_type\u001b[39m(directory: os\u001b[38;5;241m.\u001b[39mpath, file_extension: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[os\u001b[38;5;241m.\u001b[39mpath]:\n\u001b[0;32m      9\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124;03m    Lists all files with a specified file extension found in a directory.\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03m        (list[path]) A list of files with the given extension inside the directory.\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mglob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mglob\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdirectory\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/*\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mfile_extension\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\glob.py:28\u001b[0m, in \u001b[0;36mglob\u001b[1;34m(pathname, root_dir, dir_fd, recursive, include_hidden)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mglob\u001b[39m(pathname, \u001b[38;5;241m*\u001b[39m, root_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dir_fd\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, recursive\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     14\u001b[0m         include_hidden\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m     15\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a list of paths matching a pathname pattern.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03m    The pattern may contain simple shell-style wildcards a la\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;124;03m    zero or more directories and subdirectories.\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(iglob(pathname, root_dir\u001b[38;5;241m=\u001b[39mroot_dir, dir_fd\u001b[38;5;241m=\u001b[39mdir_fd, recursive\u001b[38;5;241m=\u001b[39mrecursive,\n\u001b[0;32m     29\u001b[0m                       include_hidden\u001b[38;5;241m=\u001b[39minclude_hidden))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\glob.py:97\u001b[0m, in \u001b[0;36m_iglob\u001b[1;34m(pathname, root_dir, dir_fd, recursive, dironly, include_hidden)\u001b[0m\n\u001b[0;32m     95\u001b[0m     glob_in_dir \u001b[38;5;241m=\u001b[39m _glob0\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dirname \u001b[38;5;129;01min\u001b[39;00m dirs:\n\u001b[1;32m---> 97\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[43mglob_in_dir\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_join\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdirname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbasename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdir_fd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdironly\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     98\u001b[0m \u001b[43m                           \u001b[49m\u001b[43minclude_hidden\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_hidden\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     99\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dirname, name)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\glob.py:106\u001b[0m, in \u001b[0;36m_glob1\u001b[1;34m(dirname, pattern, dir_fd, dironly, include_hidden)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_glob1\u001b[39m(dirname, pattern, dir_fd, dironly, include_hidden\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m--> 106\u001b[0m     names \u001b[38;5;241m=\u001b[39m \u001b[43m_listdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdir_fd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdironly\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m include_hidden \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _ishidden(pattern):\n\u001b[0;32m    108\u001b[0m         names \u001b[38;5;241m=\u001b[39m (x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m names \u001b[38;5;28;01mif\u001b[39;00m include_hidden \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _ishidden(x))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\glob.py:177\u001b[0m, in \u001b[0;36m_listdir\u001b[1;34m(dirname, dir_fd, dironly)\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_listdir\u001b[39m(dirname, dir_fd, dironly):\n\u001b[0;32m    176\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mclosing(_iterdir(dirname, dir_fd, dironly)) \u001b[38;5;28;01mas\u001b[39;00m it:\n\u001b[1;32m--> 177\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(it)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\glob.py:166\u001b[0m, in \u001b[0;36m_iterdir\u001b[1;34m(dirname, dir_fd, dironly)\u001b[0m\n\u001b[0;32m    164\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m fsencode(entry\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m    165\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 166\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m entry\u001b[38;5;241m.\u001b[39mname\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from data_preparation.dataset_preparation import TauPreparer\n",
    "in_root_dir = \"M:/Unused/TauCellDL\"\n",
    "\n",
    "data_preparer = TauPreparer(in_root_dir=in_root_dir, in_img_dir=\"images\", in_label_dir=\"labels\",\n",
    "                            prepared_root_dir=in_root_dir, prepared_img_dir=\"images_new\", prepared_label_dir=\"labels_new\")\n",
    "data_preparer.prepare_labels_for_yolo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TauPreparer' object has no attribute 'separate_tiles_with_cut_log'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[116], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdata_preparer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseparate_tiles_with_cut_log\u001b[49m()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'TauPreparer' object has no attribute 'separate_tiles_with_cut_log'"
     ]
    }
   ],
   "source": [
    "data_preparer.separate_tiles_with_cut_log()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "from data_preparation import data_utils, image_labelling\n",
    "\n",
    "\n",
    "def count_objects(dataset_preparer):\n",
    "        \"\"\"\n",
    "        Print the number of images in each directory in the prepared image directory,\n",
    "            along with the number of objects per image.\n",
    "        \"\"\"\n",
    "        root_img_dir = \"M:/Unused/TauCellDL/images\"\n",
    "        root_label_dir = os.path.join(\n",
    "            data_preparer.prepared_root_dir, data_preparer.prepared_label_dir)\n",
    "\n",
    "        sets = [f for f in os.listdir(root_img_dir)]\n",
    "\n",
    "        for set_name in sets:\n",
    "            num_objects_per_class = defaultdict(int)\n",
    "\n",
    "            img_dir = os.path.join(root_img_dir, set_name)\n",
    "            label_dir = os.path.join(root_label_dir, set_name)\n",
    "\n",
    "            img_paths = data_utils.list_files_of_a_type(img_dir, \".png\")\n",
    "\n",
    "            for img_path in img_paths:\n",
    "                img_name = data_utils.get_filename(img_path)\n",
    "                label_path = os.path.join(label_dir, img_name + \".txt\")\n",
    "\n",
    "                bboxes, labels = image_labelling.bboxes_from_yolo_labels(\n",
    "                    label_path)\n",
    "\n",
    "                for l in labels:\n",
    "                    num_objects_per_class[l] += 1\n",
    "\n",
    "            print(\n",
    "                f\"Objects per class in set {set_name} : {num_objects_per_class}\")\n",
    "            print(f\"Number of images in set {set_name}: {len(img_paths)}\")\n",
    "            num_objects_per_class.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = {\n",
    "    'BG': {},\n",
    "    'cortical': {},\n",
    "    'DN': {}\n",
    "}\n",
    "from data_preparation import data_utils\n",
    "import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide_to_area = {\n",
    "    '703488': 'BG',\n",
    "    '747821': 'BG',\n",
    "    '747331': 'cortical',\n",
    "    '771747': 'cortical',\n",
    "    '747335': 'DN',\n",
    "    '771913': 'DN'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing M:/Tanrada/Validation/Annotated-files/Held-out_test-set/Cortical\\747331_Annelies_.txt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 1/4 [00:00<00:01,  2.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing M:/Tanrada/Validation/Annotated-files/Held-out_test-set/Cortical\\747331_Tanrada_5%_May.txt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 2/4 [00:00<00:00,  2.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing M:/Tanrada/Validation/Annotated-files/Held-out_test-set/Cortical\\771747_Annelies.txt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 3/4 [00:16<00:07,  7.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing M:/Tanrada/Validation/Annotated-files/Held-out_test-set/Cortical\\771747_Tanrada_5%_May.txt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:37<00:00,  9.38s/it]\n"
     ]
    }
   ],
   "source": [
    "def separate_tiles_with_cut_log():\n",
    "        detection_files = data_utils.list_files_of_a_type(\"M:/Tanrada/Validation/Annotated-files/Held-out_test-set/Cortical\", \".txt\")\n",
    "\n",
    "        for i in tqdm.tqdm(range(len(detection_files))):\n",
    "            detection_file = detection_files[i]\n",
    "            print(f\"\\nProcessing {detection_file}...\")\n",
    "            with open(detection_file, 'r') as f:   \n",
    "                slide_id = data_utils.get_filename(detection_file).split('.')[0].split('_')[0]\n",
    "                area = slide_to_area[slide_id]\n",
    "                for line in f:\n",
    "                    if line.split(':')[0].strip().strip(']').strip('[') != \"Unlabelled\":\n",
    "                        if line.split(':')[0].strip().strip(']').strip('[') not in dict[area]:\n",
    "                            dict[area][line.split(':')[0].strip().strip(']').strip('[')] = 1\n",
    "                        else:\n",
    "                            dict[area][line.split(':')[0].strip().strip(']').strip('[')] += 1\n",
    "separate_tiles_with_cut_log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M:/Tanrada/Validation/Training_DN/data\\377\\server.json\n",
      "M:/Tanrada/Validation/Training_DN/data\\378\\server.json\n"
     ]
    }
   ],
   "source": [
    "# search all subdirectories in M:\\Tanrada\\Validation\\Training_DN\\data, and see if any server.json file contains the string 'svs_Tanrada_2'\n",
    "import os\n",
    "\n",
    "root_dir = \"M:/Tanrada/Validation/Training_DN/data\"\n",
    "\n",
    "for root, dirs, files in os.walk(root_dir):\n",
    "    for file in files:\n",
    "        if file.endswith(\"server.json\"):\n",
    "            with open(os.path.join(root, file), 'r') as f:\n",
    "                if \"svs_Tanrada_2\" in f.read():\n",
    "                    print(os.path.join(root, file))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "find 1 train slide for each brain region (BG, cortical, dn) that contains the object distribution closest to the test distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_train = pd.read_excel(\"M:/Unused/TauCellDL/statistics (version 1).xlsx\", sheet_name=\"Sheet1\")\n",
    "df_test = pd.read_excel(\"M:/Unused/TauCellDL/object_size_statistics (version 1).xlsx\", sheet_name=\"Sheet2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>slide_id</th>\n",
       "      <th>class</th>\n",
       "      <th>kept</th>\n",
       "      <th>Region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>747297</td>\n",
       "      <td>TA</td>\n",
       "      <td>11</td>\n",
       "      <td>bg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>747297</td>\n",
       "      <td>CB</td>\n",
       "      <td>157</td>\n",
       "      <td>bg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>747297</td>\n",
       "      <td>NFT</td>\n",
       "      <td>64</td>\n",
       "      <td>bg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>747297</td>\n",
       "      <td>Tau_fragments</td>\n",
       "      <td>1023</td>\n",
       "      <td>bg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>747309</td>\n",
       "      <td>TA</td>\n",
       "      <td>155</td>\n",
       "      <td>bg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>747309</td>\n",
       "      <td>CB</td>\n",
       "      <td>381</td>\n",
       "      <td>bg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>747309</td>\n",
       "      <td>NFT</td>\n",
       "      <td>19</td>\n",
       "      <td>bg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>747309</td>\n",
       "      <td>Tau_fragments</td>\n",
       "      <td>5743</td>\n",
       "      <td>bg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>747316</td>\n",
       "      <td>TA</td>\n",
       "      <td>134</td>\n",
       "      <td>cortical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>747316</td>\n",
       "      <td>CB</td>\n",
       "      <td>257</td>\n",
       "      <td>cortical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>747316</td>\n",
       "      <td>NFT</td>\n",
       "      <td>71</td>\n",
       "      <td>cortical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>747316</td>\n",
       "      <td>Tau_fragments</td>\n",
       "      <td>1384</td>\n",
       "      <td>cortical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>747337</td>\n",
       "      <td>TA</td>\n",
       "      <td>0</td>\n",
       "      <td>dn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>747337</td>\n",
       "      <td>CB</td>\n",
       "      <td>54</td>\n",
       "      <td>dn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>747337</td>\n",
       "      <td>NFT</td>\n",
       "      <td>161</td>\n",
       "      <td>dn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>747337</td>\n",
       "      <td>Tau_fragments</td>\n",
       "      <td>697</td>\n",
       "      <td>dn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>747350</td>\n",
       "      <td>TA</td>\n",
       "      <td>0</td>\n",
       "      <td>dn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>747350</td>\n",
       "      <td>CB</td>\n",
       "      <td>29</td>\n",
       "      <td>dn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>747350</td>\n",
       "      <td>NFT</td>\n",
       "      <td>44</td>\n",
       "      <td>dn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>747350</td>\n",
       "      <td>Tau_fragments</td>\n",
       "      <td>141</td>\n",
       "      <td>dn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>747352</td>\n",
       "      <td>TA</td>\n",
       "      <td>0</td>\n",
       "      <td>dn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>747352</td>\n",
       "      <td>CB</td>\n",
       "      <td>92</td>\n",
       "      <td>dn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>747352</td>\n",
       "      <td>NFT</td>\n",
       "      <td>131</td>\n",
       "      <td>dn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>747352</td>\n",
       "      <td>Tau_fragments</td>\n",
       "      <td>1165</td>\n",
       "      <td>dn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>747814</td>\n",
       "      <td>TA</td>\n",
       "      <td>114</td>\n",
       "      <td>bg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>747814</td>\n",
       "      <td>CB</td>\n",
       "      <td>228</td>\n",
       "      <td>bg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>747814</td>\n",
       "      <td>NFT</td>\n",
       "      <td>60</td>\n",
       "      <td>bg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>747814</td>\n",
       "      <td>Tau_fragments</td>\n",
       "      <td>3680</td>\n",
       "      <td>bg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>747818</td>\n",
       "      <td>TA</td>\n",
       "      <td>25</td>\n",
       "      <td>bg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>747818</td>\n",
       "      <td>CB</td>\n",
       "      <td>400</td>\n",
       "      <td>bg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>747818</td>\n",
       "      <td>NFT</td>\n",
       "      <td>56</td>\n",
       "      <td>bg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>747818</td>\n",
       "      <td>Tau_fragments</td>\n",
       "      <td>7202</td>\n",
       "      <td>bg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>771746</td>\n",
       "      <td>TA</td>\n",
       "      <td>71</td>\n",
       "      <td>cortical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>771746</td>\n",
       "      <td>CB</td>\n",
       "      <td>170</td>\n",
       "      <td>cortical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>771746</td>\n",
       "      <td>NFT</td>\n",
       "      <td>38</td>\n",
       "      <td>cortical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>771746</td>\n",
       "      <td>Tau_fragments</td>\n",
       "      <td>537</td>\n",
       "      <td>cortical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>771791</td>\n",
       "      <td>TA</td>\n",
       "      <td>164</td>\n",
       "      <td>cortical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>771791</td>\n",
       "      <td>CB</td>\n",
       "      <td>348</td>\n",
       "      <td>cortical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>771791</td>\n",
       "      <td>NFT</td>\n",
       "      <td>57</td>\n",
       "      <td>cortical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>771791</td>\n",
       "      <td>Tau_fragments</td>\n",
       "      <td>1246</td>\n",
       "      <td>cortical</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    slide_id          class  kept    Region\n",
       "0     747297             TA    11        bg\n",
       "1     747297             CB   157        bg\n",
       "2     747297            NFT    64        bg\n",
       "3     747297  Tau_fragments  1023        bg\n",
       "4     747309             TA   155        bg\n",
       "5     747309             CB   381        bg\n",
       "6     747309            NFT    19        bg\n",
       "7     747309  Tau_fragments  5743        bg\n",
       "8     747316             TA   134  cortical\n",
       "9     747316             CB   257  cortical\n",
       "10    747316            NFT    71  cortical\n",
       "11    747316  Tau_fragments  1384  cortical\n",
       "12    747337             TA     0        dn\n",
       "13    747337             CB    54        dn\n",
       "14    747337            NFT   161        dn\n",
       "15    747337  Tau_fragments   697        dn\n",
       "16    747350             TA     0        dn\n",
       "17    747350             CB    29        dn\n",
       "18    747350            NFT    44        dn\n",
       "19    747350  Tau_fragments   141        dn\n",
       "20    747352             TA     0        dn\n",
       "21    747352             CB    92        dn\n",
       "22    747352            NFT   131        dn\n",
       "23    747352  Tau_fragments  1165        dn\n",
       "24    747814             TA   114        bg\n",
       "25    747814             CB   228        bg\n",
       "26    747814            NFT    60        bg\n",
       "27    747814  Tau_fragments  3680        bg\n",
       "28    747818             TA    25        bg\n",
       "29    747818             CB   400        bg\n",
       "30    747818            NFT    56        bg\n",
       "31    747818  Tau_fragments  7202        bg\n",
       "32    771746             TA    71  cortical\n",
       "33    771746             CB   170  cortical\n",
       "34    771746            NFT    38  cortical\n",
       "35    771746  Tau_fragments   537  cortical\n",
       "36    771791             TA   164  cortical\n",
       "37    771791             CB   348  cortical\n",
       "38    771791            NFT    57  cortical\n",
       "39    771791  Tau_fragments  1246  cortical"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>BG</th>\n",
       "      <th>Cortical</th>\n",
       "      <th>DN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb</td>\n",
       "      <td>159</td>\n",
       "      <td>296</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NFT</td>\n",
       "      <td>21</td>\n",
       "      <td>78</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TA</td>\n",
       "      <td>49</td>\n",
       "      <td>237</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tau_fragments</td>\n",
       "      <td>2796</td>\n",
       "      <td>1761</td>\n",
       "      <td>845</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0    BG  Cortical   DN\n",
       "0             cb   159       296   18\n",
       "1            NFT    21        78   32\n",
       "2             TA    49       237    0\n",
       "3  tau_fragments  2796      1761  845"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Region: BG\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Slide: 747297\n",
      "Similarity Scores - Train:Test: 0.999, Val:Test: 0.994\n",
      "\n",
      "Class          Train:Val:Test                Raw Counts (Train/Val/Test)\n",
      "---------------------------------------------------------------------------\n",
      "CB            12.3:1.0:1           (1959/157/159)\n",
      "NFT           30.3:3.0:1           (637/64/21)\n",
      "TA            13.5:0.2:1           (663/11/49)\n",
      "Tau_fragments 7.8:0.4:1            (21795/1023/2796)\n",
      "\n",
      "Total Counts - Train: 25054, Val: 1255, Test: 3025\n",
      "Overall Ratio - 8.3:0.4:1\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Slide: 747309\n",
      "Similarity Scores - Train:Test: 0.998, Val:Test: 1.000\n",
      "\n",
      "Class          Train:Val:Test                Raw Counts (Train/Val/Test)\n",
      "---------------------------------------------------------------------------\n",
      "CB            10.9:2.4:1           (1735/381/159)\n",
      "NFT           32.5:0.9:1           (682/19/21)\n",
      "TA            10.6:3.2:1           (519/155/49)\n",
      "Tau_fragments 6.1:2.1:1            (17075/5743/2796)\n",
      "\n",
      "Total Counts - Train: 20011, Val: 6298, Test: 3025\n",
      "Overall Ratio - 6.6:2.1:1\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Slide: 747814\n",
      "Similarity Scores - Train:Test: 0.999, Val:Test: 1.000\n",
      "\n",
      "Class          Train:Val:Test                Raw Counts (Train/Val/Test)\n",
      "---------------------------------------------------------------------------\n",
      "CB            11.9:1.4:1           (1888/228/159)\n",
      "NFT           30.5:2.9:1           (641/60/21)\n",
      "TA            11.4:2.3:1           (560/114/49)\n",
      "Tau_fragments 6.8:1.3:1            (19138/3680/2796)\n",
      "\n",
      "Total Counts - Train: 22227, Val: 4082, Test: 3025\n",
      "Overall Ratio - 7.3:1.3:1\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Slide: 747818\n",
      "Similarity Scores - Train:Test: 0.998, Val:Test: 1.000\n",
      "\n",
      "Class          Train:Val:Test                Raw Counts (Train/Val/Test)\n",
      "---------------------------------------------------------------------------\n",
      "CB            10.8:2.5:1           (1716/400/159)\n",
      "NFT           30.7:2.7:1           (645/56/21)\n",
      "TA            13.2:0.5:1           (649/25/49)\n",
      "Tau_fragments 5.6:2.6:1            (15616/7202/2796)\n",
      "\n",
      "Total Counts - Train: 18626, Val: 7683, Test: 3025\n",
      "Overall Ratio - 6.2:2.5:1\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Region: CORTICAL\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Slide: 747316\n",
      "Similarity Scores - Train:Test: 0.999, Val:Test: 0.988\n",
      "\n",
      "Class          Train:Val:Test                Raw Counts (Train/Val/Test)\n",
      "---------------------------------------------------------------------------\n",
      "CB            11.7:1.6:1           (1859/257/159)\n",
      "NFT           30.0:3.4:1           (630/71/21)\n",
      "TA            11.0:2.7:1           (540/134/49)\n",
      "Tau_fragments 7.7:0.5:1            (21434/1384/2796)\n",
      "\n",
      "Total Counts - Train: 24463, Val: 1846, Test: 3025\n",
      "Overall Ratio - 8.1:0.6:1\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Slide: 771746\n",
      "Similarity Scores - Train:Test: 0.999, Val:Test: 0.962\n",
      "\n",
      "Class          Train:Val:Test                Raw Counts (Train/Val/Test)\n",
      "---------------------------------------------------------------------------\n",
      "CB            12.2:1.1:1           (1946/170/159)\n",
      "NFT           31.6:1.8:1           (663/38/21)\n",
      "TA            12.3:1.4:1           (603/71/49)\n",
      "Tau_fragments 8.0:0.2:1            (22281/537/2796)\n",
      "\n",
      "Total Counts - Train: 25493, Val: 816, Test: 3025\n",
      "Overall Ratio - 8.4:0.3:1\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Slide: 771791\n",
      "Similarity Scores - Train:Test: 0.999, Val:Test: 0.971\n",
      "\n",
      "Class          Train:Val:Test                Raw Counts (Train/Val/Test)\n",
      "---------------------------------------------------------------------------\n",
      "CB            11.1:2.2:1           (1768/348/159)\n",
      "NFT           30.7:2.7:1           (644/57/21)\n",
      "TA            10.4:3.3:1           (510/164/49)\n",
      "Tau_fragments 7.7:0.4:1            (21572/1246/2796)\n",
      "\n",
      "Total Counts - Train: 24494, Val: 1815, Test: 3025\n",
      "Overall Ratio - 8.1:0.6:1\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Region: DN\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Slide: 747337\n",
      "Similarity Scores - Train:Test: 0.999, Val:Test: 0.976\n",
      "\n",
      "Class          Train:Val:Test                Raw Counts (Train/Val/Test)\n",
      "---------------------------------------------------------------------------\n",
      "CB            13.0:0.3:1           (2062/54/159)\n",
      "NFT           25.7:7.7:1           (540/161/21)\n",
      "TA            13.8:0.0:1           (674/0/49)\n",
      "Tau_fragments 7.9:0.2:1            (22121/697/2796)\n",
      "\n",
      "Total Counts - Train: 25397, Val: 912, Test: 3025\n",
      "Overall Ratio - 8.4:0.3:1\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Slide: 747350\n",
      "Similarity Scores - Train:Test: 0.999, Val:Test: 0.948\n",
      "\n",
      "Class          Train:Val:Test                Raw Counts (Train/Val/Test)\n",
      "---------------------------------------------------------------------------\n",
      "CB            13.1:0.2:1           (2087/29/159)\n",
      "NFT           31.3:2.1:1           (657/44/21)\n",
      "TA            13.8:0.0:1           (674/0/49)\n",
      "Tau_fragments 8.1:0.1:1            (22677/141/2796)\n",
      "\n",
      "Total Counts - Train: 26095, Val: 214, Test: 3025\n",
      "Overall Ratio - 8.6:0.1:1\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Slide: 747352\n",
      "Similarity Scores - Train:Test: 0.999, Val:Test: 0.994\n",
      "\n",
      "Class          Train:Val:Test                Raw Counts (Train/Val/Test)\n",
      "---------------------------------------------------------------------------\n",
      "CB            12.7:0.6:1           (2024/92/159)\n",
      "NFT           27.1:6.2:1           (570/131/21)\n",
      "TA            13.8:0.0:1           (674/0/49)\n",
      "Tau_fragments 7.7:0.4:1            (21653/1165/2796)\n",
      "\n",
      "Total Counts - Train: 24921, Val: 1388, Test: 3025\n",
      "Overall Ratio - 8.2:0.5:1\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Best validation slide for bg: 747814\n",
      "Similarity Scores - Train:Test: 0.999, Val:Test: 1.000\n",
      "\n",
      "Ratios for best slide (Train:Val:Test):\n",
      "CB: 11.9:1.4:1\n",
      "NFT: 30.5:2.9:1\n",
      "TA: 11.4:2.3:1\n",
      "Tau_fragments: 6.8:1.3:1\n",
      "\n",
      "Best validation slide for cortical: 747316\n",
      "Similarity Scores - Train:Test: 0.999, Val:Test: 0.988\n",
      "\n",
      "Ratios for best slide (Train:Val:Test):\n",
      "CB: 11.7:1.6:1\n",
      "NFT: 30.0:3.4:1\n",
      "TA: 11.0:2.7:1\n",
      "Tau_fragments: 7.7:0.5:1\n",
      "\n",
      "Best validation slide for dn: 747352\n",
      "Similarity Scores - Train:Test: 0.999, Val:Test: 0.994\n",
      "\n",
      "Ratios for best slide (Train:Val:Test):\n",
      "CB: 12.7:0.6:1\n",
      "NFT: 27.1:6.2:1\n",
      "TA: 13.8:0.0:1\n",
      "Tau_fragments: 7.7:0.4:1\n"
     ]
    }
   ],
   "source": [
    "def calculate_vertical_ratios(df_train, df_test, val_slide):\n",
    "    \"\"\"Calculate train:val:test ratios when using a specific validation slide\"\"\"\n",
    "    # Initialize distributions\n",
    "    train_dist = {'CB': 0, 'NFT': 0, 'TA': 0, 'Tau_fragments': 0}\n",
    "    val_dist = {'CB': 0, 'NFT': 0, 'TA': 0, 'Tau_fragments': 0}\n",
    "    test_dist = {'CB': 159, 'NFT': 21, 'TA': 49, 'Tau_fragments': 2796}\n",
    "    \n",
    "    # Split data into train and validation\n",
    "    val_data = df_train[df_train['slide_id'] == val_slide]\n",
    "    train_data = df_train[df_train['slide_id'] != val_slide]\n",
    "    \n",
    "    # Calculate raw counts\n",
    "    for _, row in train_data.iterrows():\n",
    "        train_dist[row['class']] += row['kept']\n",
    "    \n",
    "    for _, row in val_data.iterrows():\n",
    "        val_dist[row['class']] += row['kept']\n",
    "    \n",
    "    # Calculate similarity scores\n",
    "    train_sim = get_distribution_similarity(train_dist, test_dist)\n",
    "    val_sim = get_distribution_similarity(val_dist, test_dist)\n",
    "    \n",
    "    # Calculate ratios for each class (relative to test set)\n",
    "    ratios = {}\n",
    "    for class_name in ['CB', 'NFT', 'TA', 'Tau_fragments']:\n",
    "        test_count = test_dist[class_name]\n",
    "        if test_count == 0:\n",
    "            ratios[class_name] = {\n",
    "                'train': 0 if train_dist[class_name] == 0 else float('inf'),\n",
    "                'val': 0 if val_dist[class_name] == 0 else float('inf'),\n",
    "                'test': 1\n",
    "            }\n",
    "        else:\n",
    "            ratios[class_name] = {\n",
    "                'train': train_dist[class_name] / test_count,\n",
    "                'val': val_dist[class_name] / test_count,\n",
    "                'test': 1  # test is our reference point\n",
    "            }\n",
    "    \n",
    "    return ratios, train_dist, val_dist, test_dist, train_sim, val_sim\n",
    "\n",
    "# Calculate distributions for each potential validation slide\n",
    "results = {}\n",
    "for region in ['bg', 'cortical', 'dn']:\n",
    "    region_slides = df_train[df_train['Region'] == region]['slide_id'].unique()\n",
    "    region_results = {}\n",
    "    \n",
    "    for slide_id in region_slides:\n",
    "        ratios, train_counts, val_counts, test_counts, train_sim, val_sim = calculate_vertical_ratios(df_train, df_test, slide_id)\n",
    "        region_results[slide_id] = {\n",
    "            'ratios': ratios,\n",
    "            'train_counts': train_counts,\n",
    "            'val_counts': val_counts,\n",
    "            'test_counts': test_counts,\n",
    "            'train_sim': train_sim,\n",
    "            'val_sim': val_sim\n",
    "        }\n",
    "    \n",
    "    results[region] = region_results\n",
    "\n",
    "# Print results\n",
    "for region, region_results in results.items():\n",
    "    print(f\"\\nRegion: {region.upper()}\")\n",
    "    print(\"-\" * 100)\n",
    "    \n",
    "    for slide_id, data in region_results.items():\n",
    "        print(f\"\\nSlide: {slide_id}\")\n",
    "        print(f\"Similarity Scores - Train:Test: {data['train_sim']:.3f}, Val:Test: {data['val_sim']:.3f}\")\n",
    "        print(\"\\nClass          Train:Val:Test                Raw Counts (Train/Val/Test)\")\n",
    "        print(\"-\" * 75)\n",
    "        \n",
    "        ratios = data['ratios']\n",
    "        total_train = 0\n",
    "        total_val = 0\n",
    "        total_test = 0\n",
    "        \n",
    "        for class_name in ['CB', 'NFT', 'TA', 'Tau_fragments']:\n",
    "            ratio_str = f\"{ratios[class_name]['train']:.1f}:{ratios[class_name]['val']:.1f}:1\"\n",
    "            counts_str = f\"{data['train_counts'][class_name]}/{data['val_counts'][class_name]}/{data['test_counts'][class_name]}\"\n",
    "            print(f\"{class_name:<13} {ratio_str:<20} ({counts_str})\")\n",
    "            \n",
    "            total_train += data['train_counts'][class_name]\n",
    "            total_val += data['val_counts'][class_name]\n",
    "            total_test += data['test_counts'][class_name]\n",
    "        \n",
    "        print(f\"\\nTotal Counts - Train: {total_train}, Val: {total_val}, Test: {total_test}\")\n",
    "        print(f\"Overall Ratio - {total_train/total_test:.1f}:{total_val/total_test:.1f}:1\")\n",
    "        print(\"-\" * 75)\n",
    "\n",
    "# Find best validation slide based on combination of ratio and similarity scores\n",
    "def calculate_combined_score(data, target_train=4.7, target_val=1):\n",
    "    \"\"\"Calculate score based on both ratio proximity and distribution similarity\"\"\"\n",
    "    ratio_score = 0\n",
    "    for class_ratios in data['ratios'].values():\n",
    "        if class_ratios['train'] != float('inf'):\n",
    "            ratio_score -= abs(class_ratios['train'] - target_train)\n",
    "        if class_ratios['val'] != float('inf'):\n",
    "            ratio_score -= abs(class_ratios['val'] - target_val)\n",
    "    \n",
    "    similarity_score = (data['train_sim'] + data['val_sim']) / 2\n",
    "    return ratio_score + similarity_score\n",
    "\n",
    "for region, region_results in results.items():\n",
    "    best_slide = max(region_results.items(), \n",
    "                    key=lambda x: calculate_combined_score(x[1]))\n",
    "    \n",
    "    print(f\"\\nBest validation slide for {region}: {best_slide[0]}\")\n",
    "    print(f\"Similarity Scores - Train:Test: {best_slide[1]['train_sim']:.3f}, Val:Test: {best_slide[1]['val_sim']:.3f}\")\n",
    "    print(\"\\nRatios for best slide (Train:Val:Test):\")\n",
    "    ratios = best_slide[1]['ratios']\n",
    "    for class_name in ['CB', 'NFT', 'TA', 'Tau_fragments']:\n",
    "        ratio_str = f\"{ratios[class_name]['train']:.1f}:{ratios[class_name]['val']:.1f}:1\"\n",
    "        print(f\"{class_name}: {ratio_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Distribution Comparison\n",
      "====================================================================================================\n",
      "Option 1: {'747352', '747316', '747309'}\n",
      "Option 2: {'747814', '747352', '747316'}\n",
      "\n",
      "Class Distribution (Train:Val:Test)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Class         Option 1                            Option 2                            Test\n",
      "----------------------------------------------------------------------------------------------------\n",
      "CB            8.7:4.6:1       (1386/730 ) 9.7:3.6:1       (1539/577 )  159\n",
      "NFT           22.9:10.5:1     ( 480/221 ) 20.9:12.5:1     ( 439/262 )   21\n",
      "TA            7.9:5.9:1       ( 385/289 ) 8.7:5.1:1       ( 426/248 )   49\n",
      "Tau_fragments 5.2:3.0:1       (14526/8292) 5.9:2.2:1       (16589/6229) 2796\n",
      "\n",
      "Total Counts and Ratios:\n",
      "Option 1 - Train: 16777, Val: 9532, Ratio: 5.5:3.2:1\n",
      "Option 2 - Train: 18993, Val: 7316, Ratio: 6.3:2.4:1\n",
      "Test: 3025\n",
      "\n",
      "Distribution Similarity Scores:\n",
      "Option 1 - Train-Test: 0.999, Val-Test: 0.999\n",
      "Option 2 - Train-Test: 0.999, Val-Test: 0.999\n"
     ]
    }
   ],
   "source": [
    "def compare_split_distributions(df_train, df_test, validation_slides_1, validation_slides_2):\n",
    "    \"\"\"Compare distributions for two different validation slide combinations\"\"\"\n",
    "    def get_distributions(validation_slides):\n",
    "        train_dist = {'CB': 0, 'NFT': 0, 'TA': 0, 'Tau_fragments': 0}\n",
    "        val_dist = {'CB': 0, 'NFT': 0, 'TA': 0, 'Tau_fragments': 0}\n",
    "        \n",
    "        for _, row in df_train.iterrows():\n",
    "            slide_id = str(row['slide_id'])\n",
    "            if slide_id in validation_slides:\n",
    "                val_dist[row['class']] += row['kept']\n",
    "            else:\n",
    "                train_dist[row['class']] += row['kept']\n",
    "                \n",
    "        return train_dist, val_dist\n",
    "    \n",
    "    test_dist = {'CB': 159, 'NFT': 21, 'TA': 49, 'Tau_fragments': 2796}\n",
    "    \n",
    "    # Get distributions for both combinations\n",
    "    train_dist_1, val_dist_1 = get_distributions(validation_slides_1)\n",
    "    train_dist_2, val_dist_2 = get_distributions(validation_slides_2)\n",
    "    \n",
    "    # Print comparison\n",
    "    print(\"\\nDistribution Comparison\")\n",
    "    print(\"=\" * 100)\n",
    "    print(f\"Option 1: {validation_slides_1}\")\n",
    "    print(f\"Option 2: {validation_slides_2}\")\n",
    "    print(\"\\nClass Distribution (Train:Val:Test)\")\n",
    "    print(\"-\" * 100)\n",
    "    \n",
    "    print(f\"{'Class':<13} {'Option 1':<35} {'Option 2':<35} {'Test'}\")\n",
    "    print(\"-\" * 100)\n",
    "    \n",
    "    totals_1 = {'train': 0, 'val': 0}\n",
    "    totals_2 = {'train': 0, 'val': 0}\n",
    "    total_test = 0\n",
    "    \n",
    "    for class_name in ['CB', 'NFT', 'TA', 'Tau_fragments']:\n",
    "        train_1 = train_dist_1[class_name]\n",
    "        val_1 = val_dist_1[class_name]\n",
    "        train_2 = train_dist_2[class_name]\n",
    "        val_2 = val_dist_2[class_name]\n",
    "        test = test_dist[class_name]\n",
    "        \n",
    "        totals_1['train'] += train_1\n",
    "        totals_1['val'] += val_1\n",
    "        totals_2['train'] += train_2\n",
    "        totals_2['val'] += val_2\n",
    "        total_test += test\n",
    "        \n",
    "        ratio_1 = f\"{train_1/test:.1f}:{val_1/test:.1f}:1\" if test > 0 else \"N/A\"\n",
    "        ratio_2 = f\"{train_2/test:.1f}:{val_2/test:.1f}:1\" if test > 0 else \"N/A\"\n",
    "        \n",
    "        print(f\"{class_name:<13} {ratio_1:<15} ({train_1:>4d}/{val_1:<4d}) {ratio_2:<15} ({train_2:>4d}/{val_2:<4d}) {test:>4d}\")\n",
    "    \n",
    "    print(\"\\nTotal Counts and Ratios:\")\n",
    "    print(f\"Option 1 - Train: {totals_1['train']}, Val: {totals_1['val']}, Ratio: {totals_1['train']/total_test:.1f}:{totals_1['val']/total_test:.1f}:1\")\n",
    "    print(f\"Option 2 - Train: {totals_2['train']}, Val: {totals_2['val']}, Ratio: {totals_2['train']/total_test:.1f}:{totals_2['val']/total_test:.1f}:1\")\n",
    "    print(f\"Test: {total_test}\")\n",
    "    \n",
    "    # Calculate distribution similarities\n",
    "    train_sim_1 = get_distribution_similarity(train_dist_1, test_dist)\n",
    "    val_sim_1 = get_distribution_similarity(val_dist_1, test_dist)\n",
    "    train_sim_2 = get_distribution_similarity(train_dist_2, test_dist)\n",
    "    val_sim_2 = get_distribution_similarity(val_dist_2, test_dist)\n",
    "    \n",
    "    print(\"\\nDistribution Similarity Scores:\")\n",
    "    print(f\"Option 1 - Train-Test: {train_sim_1:.3f}, Val-Test: {val_sim_1:.3f}\")\n",
    "    print(f\"Option 2 - Train-Test: {train_sim_2:.3f}, Val-Test: {val_sim_2:.3f}\")\n",
    "\n",
    "# Compare the two options\n",
    "validation_slides_1 = {'747352', '747316', '747309'}  # Original\n",
    "validation_slides_2 = {'747352', '747316', '747814'}  # New option\n",
    "compare_split_distributions(df_train, df_test, validation_slides_1, validation_slides_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for optimal combination...\n",
      "\n",
      "BEST COMBINATION FOUND\n",
      "================================================================================\n",
      "Validation slides: {'747337', '771746', '747297'}\n",
      "\n",
      "Class Distribution (Train:Val:Test)\n",
      "--------------------------------------------------\n",
      "CB            10.9:2.4:1           (1735/381/159)\n",
      "NFT           20.9:12.5:1          (438/263/21)\n",
      "TA            12.1:1.7:1           (592/82/49)\n",
      "Tau_fragments 7.4:0.8:1            (20561/2257/2796)\n",
      "\n",
      "Total Counts:\n",
      "Train: 23326\n",
      "Val:   2983\n",
      "Test:  3025\n",
      "\n",
      "Overall Ratio (Train:Val:Test): 7.7:1.0:1\n",
      "\n",
      "Distribution Similarity Scores:\n",
      "Train-Test: 0.999\n",
      "Val-Test:   0.988\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "\n",
    "def calculate_split_scores(df_train, df_test, validation_slides):\n",
    "    \"\"\"Calculate how close the split is to 8:1:1 for each class\"\"\"\n",
    "    train_dist = {'CB': 0, 'NFT': 0, 'TA': 0, 'Tau_fragments': 0}\n",
    "    val_dist = {'CB': 0, 'NFT': 0, 'TA': 0, 'Tau_fragments': 0}\n",
    "    test_dist = {'CB': 159, 'NFT': 21, 'TA': 49, 'Tau_fragments': 2796}\n",
    "    \n",
    "    # Split data into train and validation\n",
    "    for _, row in df_train.iterrows():\n",
    "        slide_id = str(row['slide_id'])\n",
    "        if slide_id in validation_slides:\n",
    "            val_dist[row['class']] += row['kept']\n",
    "        else:\n",
    "            train_dist[row['class']] += row['kept']\n",
    "    \n",
    "    # Calculate score based on deviation from 8:1:1 ratio\n",
    "    score = 0\n",
    "    total_train = sum(train_dist.values())\n",
    "    total_val = sum(val_dist.values())\n",
    "    total_test = sum(test_dist.values())\n",
    "    \n",
    "    # Score for overall ratio\n",
    "    target_train_ratio = 8\n",
    "    target_val_ratio = 1\n",
    "    if total_test > 0:\n",
    "        actual_train_ratio = total_train / total_test\n",
    "        actual_val_ratio = total_val / total_test\n",
    "        score -= abs(actual_train_ratio - target_train_ratio)\n",
    "        score -= abs(actual_val_ratio - target_val_ratio)\n",
    "    \n",
    "    # Score for individual class ratios\n",
    "    for class_name in train_dist.keys():\n",
    "        if test_dist[class_name] > 0:\n",
    "            class_train_ratio = train_dist[class_name] / test_dist[class_name]\n",
    "            class_val_ratio = val_dist[class_name] / test_dist[class_name]\n",
    "            score -= abs(class_train_ratio - target_train_ratio)\n",
    "            score -= abs(class_val_ratio - target_val_ratio)\n",
    "    \n",
    "    return score, train_dist, val_dist, test_dist\n",
    "\n",
    "# Get all slides by region\n",
    "bg_slides = df_train[df_train['Region'] == 'bg']['slide_id'].unique()\n",
    "cortical_slides = df_train[df_train['Region'] == 'cortical']['slide_id'].unique()\n",
    "dn_slides = df_train[df_train['Region'] == 'dn']['slide_id'].unique()\n",
    "\n",
    "# Try all combinations\n",
    "best_score = float('-inf')\n",
    "best_combination = None\n",
    "best_distributions = None\n",
    "\n",
    "print(\"Searching for optimal combination...\")\n",
    "for bg, cortical, dn in product(bg_slides, cortical_slides, dn_slides):\n",
    "    val_slides = {str(bg), str(cortical), str(dn)}\n",
    "    score, train_dist, val_dist, test_dist = calculate_split_scores(df_train, df_test, val_slides)\n",
    "    \n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_combination = val_slides\n",
    "        best_distributions = (train_dist, val_dist, test_dist)\n",
    "\n",
    "# Print results\n",
    "print(\"\\nBEST COMBINATION FOUND\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Validation slides: {best_combination}\")\n",
    "print(\"\\nClass Distribution (Train:Val:Test)\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "train_dist, val_dist, test_dist = best_distributions\n",
    "total_train = 0\n",
    "total_val = 0\n",
    "total_test = 0\n",
    "\n",
    "for class_name in ['CB', 'NFT', 'TA', 'Tau_fragments']:\n",
    "    train_count = train_dist[class_name]\n",
    "    val_count = val_dist[class_name]\n",
    "    test_count = test_dist[class_name]\n",
    "    \n",
    "    total_train += train_count\n",
    "    total_val += val_count\n",
    "    total_test += test_count\n",
    "    \n",
    "    if test_count == 0:\n",
    "        ratio_str = \"inf:inf:1\" if train_count > 0 or val_count > 0 else \"0:0:0\"\n",
    "    else:\n",
    "        ratio_str = f\"{train_count/test_count:.1f}:{val_count/test_count:.1f}:1\"\n",
    "        \n",
    "    print(f\"{class_name:<13} {ratio_str:<20} ({train_count}/{val_count}/{test_count})\")\n",
    "\n",
    "print(\"\\nTotal Counts:\")\n",
    "print(f\"Train: {total_train}\")\n",
    "print(f\"Val:   {total_val}\")\n",
    "print(f\"Test:  {total_test}\")\n",
    "\n",
    "overall_ratio = f\"{total_train/total_test:.1f}:{total_val/total_test:.1f}:1\"\n",
    "print(f\"\\nOverall Ratio (Train:Val:Test): {overall_ratio}\")\n",
    "\n",
    "# Calculate distribution similarity\n",
    "train_sim = get_distribution_similarity(train_dist, test_dist)\n",
    "val_sim = get_distribution_similarity(val_dist, test_dist)\n",
    "print(f\"\\nDistribution Similarity Scores:\")\n",
    "print(f\"Train-Test: {train_sim:.3f}\")\n",
    "print(f\"Val-Test:   {val_sim:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Region: bg\n",
      "Best matching slide: 747309\n",
      "Similarity score: 1.000\n",
      "\n",
      "Distributions:\n",
      "Class          Train    Test\n",
      "-----------------------------------\n",
      "CB                381     159\n",
      "NFT                19      21\n",
      "TA                155      49\n",
      "Tau_fragments    5743    2796\n",
      "\n",
      "Region: cortical\n",
      "Best matching slide: 747316\n",
      "Similarity score: 0.999\n",
      "\n",
      "Distributions:\n",
      "Class          Train    Test\n",
      "-----------------------------------\n",
      "CB                257     296\n",
      "NFT                71      78\n",
      "TA                134     237\n",
      "Tau_fragments    1384    1761\n",
      "\n",
      "Region: dn\n",
      "Best matching slide: 747352\n",
      "Similarity score: 0.996\n",
      "\n",
      "Distributions:\n",
      "Class          Train    Test\n",
      "-----------------------------------\n",
      "CB                 92      18\n",
      "NFT               131      32\n",
      "TA                  0       0\n",
      "Tau_fragments    1165     845\n"
     ]
    }
   ],
   "source": [
    "# Function to calculate cosine similarity between two distributions\n",
    "def get_distribution_similarity(dist1, dist2):\n",
    "    # Get all possible classes\n",
    "    all_classes = ['CB', 'NFT', 'TA', 'Tau_fragments']\n",
    "    \n",
    "    # Create vectors\n",
    "    vec1 = np.array([dist1.get(c, 0) for c in all_classes])\n",
    "    vec2 = np.array([dist2.get(c, 0) for c in all_classes])\n",
    "    \n",
    "    # Normalize the vectors\n",
    "    vec1 = vec1 / np.sum(vec1)\n",
    "    vec2 = vec2 / np.sum(vec2)\n",
    "    \n",
    "    # Calculate cosine similarity\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "# Process test distributions\n",
    "test_distributions = {\n",
    "    'bg': {\n",
    "        'CB': 159,\n",
    "        'NFT': 21,\n",
    "        'TA': 49,\n",
    "        'Tau_fragments': 2796\n",
    "    },\n",
    "    'cortical': {\n",
    "        'CB': 296,\n",
    "        'NFT': 78,\n",
    "        'TA': 237,\n",
    "        'Tau_fragments': 1761\n",
    "    },\n",
    "    'dn': {\n",
    "        'CB': 18,\n",
    "        'NFT': 32,\n",
    "        'TA': 0,\n",
    "        'Tau_fragments': 845\n",
    "    }\n",
    "}\n",
    "\n",
    "# Find most similar slide for each region\n",
    "results = {}\n",
    "for region in ['bg', 'cortical', 'dn']:\n",
    "    best_similarity = -1\n",
    "    best_slide = None\n",
    "    \n",
    "    # Get slides for this region\n",
    "    region_slides = df_train[df_train['Region'] == region]['slide_id'].unique()\n",
    "    \n",
    "    for slide_id in region_slides:\n",
    "        # Create distribution for this slide\n",
    "        slide_dist = {}\n",
    "        slide_data = df_train[df_train['slide_id'] == slide_id]\n",
    "        for _, row in slide_data.iterrows():\n",
    "            slide_dist[row['class']] = row['kept']\n",
    "        \n",
    "        # Calculate similarity\n",
    "        similarity = get_distribution_similarity(slide_dist, test_distributions[region])\n",
    "        \n",
    "        if similarity > best_similarity:\n",
    "            best_similarity = similarity\n",
    "            best_slide = slide_id\n",
    "    \n",
    "    results[region] = {\n",
    "        'slide_id': best_slide,\n",
    "        'similarity': best_similarity\n",
    "    }\n",
    "\n",
    "# Print results with distributions for comparison\n",
    "for region, result in results.items():\n",
    "    print(f\"\\nRegion: {region}\")\n",
    "    print(f\"Best matching slide: {result['slide_id']}\")\n",
    "    print(f\"Similarity score: {result['similarity']:.3f}\")\n",
    "    \n",
    "    # Print distributions for comparison\n",
    "    slide_data = df_train[df_train['slide_id'] == result['slide_id']]\n",
    "    print(\"\\nDistributions:\")\n",
    "    print(\"Class          Train    Test\")\n",
    "    print(\"-\" * 35)\n",
    "    for class_name in ['CB', 'NFT', 'TA', 'Tau_fragments']:\n",
    "        train_count = slide_data[slide_data['class'] == class_name]['kept'].values[0]\n",
    "        test_count = test_distributions[region][class_name]\n",
    "        print(f\"{class_name:<13} {train_count:>7d} {test_count:>7d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized vectors:\n",
      "CB            0.060  0.053\n",
      "NFT           0.003  0.007\n",
      "TA            0.025  0.016\n",
      "Tau_fragments 0.912  0.924\n",
      "\n",
      "Similarity score: 1.000\n"
     ]
    }
   ],
   "source": [
    "def get_distribution_similarity(dist1, dist2, verbose=False):\n",
    "    # Get all possible classes\n",
    "    all_classes = ['CB', 'NFT', 'TA', 'Tau_fragments']\n",
    "    \n",
    "    # Create vectors\n",
    "    vec1 = np.array([dist1.get(c, 0) for c in all_classes])\n",
    "    vec2 = np.array([dist2.get(c, 0) for c in all_classes])\n",
    "    \n",
    "    # Normalize the vectors\n",
    "    vec1_norm = vec1 / np.sum(vec1)\n",
    "    vec2_norm = vec2 / np.sum(vec2)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Normalized vectors:\")\n",
    "        for i, c in enumerate(all_classes):\n",
    "            print(f\"{c:<13} {vec1_norm[i]:.3f}  {vec2_norm[i]:.3f}\")\n",
    "    \n",
    "    # Calculate cosine similarity\n",
    "    similarity = np.dot(vec1_norm, vec2_norm) / (np.linalg.norm(vec1_norm) * np.linalg.norm(vec2_norm))\n",
    "    return similarity\n",
    "\n",
    "# Test the similarity calculation for 747309\n",
    "train_dist = {\n",
    "    'CB': 381,\n",
    "    'NFT': 19,\n",
    "    'TA': 155,\n",
    "    'Tau_fragments': 5743\n",
    "}\n",
    "\n",
    "test_dist = {\n",
    "    'CB': 159,\n",
    "    'NFT': 21,\n",
    "    'TA': 49,\n",
    "    'Tau_fragments': 2796\n",
    "}\n",
    "\n",
    "similarity = get_distribution_similarity(train_dist, test_dist, verbose=True)\n",
    "print(f\"\\nSimilarity score: {similarity:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
